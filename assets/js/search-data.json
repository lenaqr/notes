{
  
    
        "post0": {
            "title": "Two probabilistic programming links",
            "content": "A preprint from my former lab: Automating Involutive MCMC using Probabilistic and Differentiable Programming. Involutive MCMC is a recently introduced framework that expresses various common MCMC algorithms as instances of a common pattern. The authors show how this pattern is supported as a probabilistic programming abstraction in Gen, allowing users to write customized inference algorithms for probabilistic models while automating the error-prone parts and providing free invariant checks. . Also: a blog post comparing and benchmarks Dirichlet Process Gaussian mixture model in various PPLs. .",
            "url": "https://luanths.github.io/notes/ppl/2020/07/26/ppl-links.html",
            "relUrl": "/ppl/2020/07/26/ppl-links.html",
            "date": " • Jul 26, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Installing OCaml at home",
            "content": "I write OCaml code in my day job, where we’ve invested in a developer experience that works out of the box with the right tools, compilers, and editor plugins. . To use OCaml at home, there’s a bit more setup. I mostly followed the instructions in Real World Ocaml, but there were a couple places where it seemed out of date and I ended up doing something different, so I’ve written up what I did here. . I started by installing opam. Since I run Arch Linux, I did this with pacman -S opam. . I also enabled the Spacemacs layer for OCaml. . Then, following the opam instructions, I initialized the package database and installed some basic libraries and tools. . opam init eval $(opam env) opam install core utop merlin ocp-indent dune . One gotcha: utop depends on lwt, which fails to compile unless the system library libev is installed. So I had to pacman -S libev. . At this point I realized that, since I was used to running everything from Emacs, eval $(opam env) wouldn’t do what I wanted since it only activates the opam environment in my shell. After a bit of searching around, I arrived at this elisp function to activate opam in Emacs. . (defun opam-env () (interactive nil) (add-to-list &#39;exec-path (replace-regexp-in-string &quot; n &#39;&quot; &quot;&quot; (shell-command-to-string &quot;opam config var bin&quot;))) (dolist (var (car (read-from-string (shell-command-to-string &quot;opam env --sexp&quot;)))) (setenv (car var) (cadr var)))) . I suppose I could’ve also run eval $(opam env) in a shell and then launched Emacs from that shell, but this works for me. . Anyway, at this point I’m ready to start a coding project. Using dune: . dune init proj myproj --libs core . This generates a little project structure, and I can open up myproj/bin/main.ml and start coding. To build the project, I run . dune build -w . which recompiles whenever I save. .",
            "url": "https://luanths.github.io/notes/linux/2020/02/13/ocaml.html",
            "relUrl": "/linux/2020/02/13/ocaml.html",
            "date": " • Feb 13, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Putnam spheres",
            "content": "Putnam 2019 A4 asks if there exists a function f(x, y, z) from 3-dimensional space to the reals, that integrates to zero over the surface of every unit sphere, but is not identically zero. . My coworkers were discussing this problem and its generalization: what about in n-dimensional space? . We think that the answer is yes for all n, and that it’s enough for f to be an axis-aligned plane wave, that is f(x, y, z, …) = cos(kx) for some frequency k. . Surprisingly (to my intuition), all we need to do is find k such that cos(kx) integrates to zero over the unit sphere centered at zero. If it does, then it integrates to zero over any unit sphere. . This is essentially because you can make any shifted copy of cos(kx) using a linear combination of cos(kx) and sin(kx), and we know that sin(kx) integrates to zero over the unit sphere centered at zero because sin is an even function, so all we need is for cos(kx) to integrate to zero for some k. . Does such a k always exist? It seems like it should, because things are continuous as a function of k. For the particular case of n = 3, k comes out particularly clean because of a coincidence in 3 dimensions. .",
            "url": "https://luanths.github.io/notes/math/2020/01/12/putnam-spheres.html",
            "relUrl": "/math/2020/01/12/putnam-spheres.html",
            "date": " • Jan 12, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Eigenvectors from eigenvalues",
            "content": "“Eigenvectors from eigenvalues” (Quanta article, Terry Tao blog post) is a recent result showing an unexpected relationship in basic linear algebra. . I was trying to develop some intuition for this result and I initially found the proofs hard to read and understand, but after some studying I think I get it. I’ve written a quick explanation below; I think this would be much improved with pictures, but I don’t want to figure that out right now. Maybe another time. . Suppose $A$ is a covariance matrix. Let $ lambda$ be the largest eigenvalue of $A$, with eigenvector $v$. We know $ lambda$ and we’d like to learn about $v$. . Consider $ lambda I - A$ as a covariance matrix. Its variance in each direction is $ lambda$ minus the variance that $A$ has in that direction. In particular, it has zero variance in the $v$ direction. If you imagine samples from a Gaussian distribution with covariance $ lambda I - A$, it’ll be a point cloud shaped like a flat ellipse that is perpendicular to $v$. . Now if we look at this flat ellipse from the $j$-th coordinate direction, how big is its “silhouette”? That is, what is the area of the projection of the ellipse onto the $j$-th coordinate plane? . If the $j$-th axis is parallel to $v$, then it’s like looking at a pancake head-on: the area of the projection is the area of the whole ellipse. But if the $j$-th axis is perpendicular to $v$, then it’s like looking at a pancake sideways: the area of the projection is zero. . In general we have a linear combination of these two cases, so we can figure out the component of $v$ that is along the $j$-th axis by taking the ratio of the projection’s area to the ellipse’s area. And that’s effectively what the eigenvectors-from-eigenvalues formula does. . (This geometric interpretation only works with the largest eigenvalue, because variances and areas have to be positive, but the math works out regardless.) . What’s notable to me is how helpful it was to my intuition to think of $A$ as a covariance matrix, rather than as a linear transformation, which is the more usual role of matrices in linear algebra. I think this applies to many uses of matrix math I see. More on this later, maybe? .",
            "url": "https://luanths.github.io/notes/math/2020/01/06/eigenvectors.html",
            "relUrl": "/math/2020/01/06/eigenvectors.html",
            "date": " • Jan 6, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Named tensors and labeled arrays",
            "content": "NamedTensor is a proposal from Harvard NLP researchers for enhancing tensors – multi-dimensional arrays which are the central object in many deep learning frameworks – with named dimensions. For anyone who’s written code to work with multi-dimensional arrays and had to deal with knowing which axis is which and making sure they line up, I think this sort of thing can help a lot with maintainability and usability. . Many commenters on the NamedTensor post pointed out the similarity to xarray, which is an existing library that brings labeled dimensions to numpy. I’ve been using xarray recently for some work projects and I’ve found that it lets me write code that feels a lot cleaner and more robust compared to the axis twiddling I would’ve done in raw numpy. .",
            "url": "https://luanths.github.io/notes/programming/2019/01/14/namedtensor.html",
            "relUrl": "/programming/2019/01/14/namedtensor.html",
            "date": " • Jan 14, 2019"
        }
        
    
  
    
        ,"post5": {
            "title": "Rambly thoughts on probabilistic programming",
            "content": "Between having just come back from NeurIPS and met up with some of my old labmates, and watching the talks from PROBPROG 2018, I’ve gotten into thinking about probabilistic programming again. . In particular I’ve been thinking about the design of probabilistic programming frameworks, from the now popular deep-learning-focused ones such as Pyro and Tensorflow Probability, to a new project from my former lab, Gen. . Of course, my M.Eng thesis was on this topic. But looking back on it now, the proposal in my thesis seems way too complicated. (I think part of it is that I was anchored to the existing implementations of Venture in the lab, which had a lot of complicated cruft in them.) . I want to revisit this and maybe eventually produce something that I can explain to other people, but for now here are some scattered thoughts: . In my thesis there was this whole section on traces with a monolithic interface that they should satisfy. I like Gen’s approach better, where a trace is basically a plain data structure whose interpretation depends on the model program. . | In fact, I would go further. Probabilistically, there’s nothing special about tracing; it’s just a program transformation that turns a probabilistic program into a program that returns a trace data structure. This should simplify concepts a lot. . | I had this idea that stochastic procedures could have their own internal mutable state, to keep track of something like sufficient statistics. In hindsight it seems better for procedures to be stateless but allowed to mutate their inputs. You can still have a stateful procedure by making a closure. Note: mutation only plays well with inference algorithms if a mutating procedure knows how to run itself backward to undo what it did. . | Another source of complexity was for handling probabilistic programs that don’t have a joint density, such as programs that call out to an external simulator. My current opinion is that this doesn’t seem worth cluttering up the core interface for. You could work around this by, for example, tracing the external program’s use of the random number generator and making a joint density out of that. (This is basically the approach in Frank Wood’s work with the physics simulator SHERPA.) . | If the trace is just a data structure like in Gen, all the incremental graph stuff in Venture just becomes a data diffing problem. That’s neat. . | .",
            "url": "https://luanths.github.io/notes/ppl/2018/12/15/probprog.html",
            "relUrl": "/ppl/2018/12/15/probprog.html",
            "date": " • Dec 15, 2018"
        }
        
    
  
    
        ,"post6": {
            "title": "Continued rational function approximations",
            "content": "A Taylor series of a function is kind of like a decimal representation of a real number, where instead of base 10 we use &quot;base x&quot; to represent functions of x. . What happens if we replace &quot;decimal representation&quot; with &quot;continued fraction representation&quot;? . Warm-up: Taylor approximations . Say we want to approximate $y = log(1 + x)$ near x = 0. Here&#39;s a graph of this function: . &lt;matplotlib.lines.Line2D at 0x7fdc8d924908&gt; . Our first order approximation is $y = x$. This matches the slope and intercept at x = 0: . &lt;matplotlib.lines.Line2D at 0x7fdc8ac055c0&gt; . Suppose now we want more terms in the Taylor approximation. We could take derivatives and plug it into the general formula for Taylor series but here&#39;s a more heuristic approach that leads to the same result (which we will modify later). . If $ y approx x $, then $ y/x approx 1 $. Define $z_1 = y/x$ and plot it as a function of x. We find that we can get a better approximation for $z_1$ by matching its slope at x = 0: . &lt;matplotlib.lines.Line2D at 0x7fdc8abe6e48&gt; . Plugging in this approximation for $z_1$ and working backward to solve for $y$ this gives us a second order Taylor approximation. . $$ begin{align} y/x &amp;= z_1 approx 1 - x/2 y &amp;= xz_1 y &amp; approx x(1 - x/2) &amp; approx x - x^2/2 end{align} $$ &lt;matplotlib.lines.Line2D at 0x7fdc8ab5b438&gt; . We can keep going to get the third term. . $$ begin{align} y &amp; approx x + x^2(-1/2) y &amp;= x + x^2 z_2 y/x &amp;= 1 + xz_2 y/x - 1 &amp;= xz_2 (y/x - 1)/x &amp;= z_2 end{align} $$so we plot $(y/x - 1)/x$ and once again match its slope . &lt;matplotlib.lines.Line2D at 0x7fdc8aabcda0&gt; . and plug it back in to get the third order Taylor expansion . $$ begin{align} y &amp;= x + x^2 z_2 y &amp; approx x + x^2 (-1/2 + x/3) &amp;= x - x^2/2 + x^3/3 end{align} $$ &lt;matplotlib.lines.Line2D at 0x7fdc8aaa0860&gt; . And we can keep going like this to get more and more terms in the Taylor series. . To summarize what we&#39;ve been doing so far: . Approximate a function as constant + linear in x | Subtract out the constant and divide what&#39;s left by x, to get something approximately constant | Replace &quot;constant&quot; with &quot;constant + linear&quot; and repeat | . Enter continued fractions . Now to this basic procedure we&#39;re going to make one change: . Approximate a function as constant + linear in x | Subtract out the constant and divide x by what&#39;s left, to get something approximately constant | Replace &quot;constant&quot; with &quot;constant + linear&quot; and repeat | . This is inspired by continued fractions as it&#39;s analogous to how you would generate a continued fraction expansion for a real number. . Let&#39;s see what happens when we apply the modified procedure to our function from before. In the first step we have $y approx x$. Last time we went from this to $y = xz_1$, now let&#39;s do $y = x/z_1$ and plot $z_1 = x/y$: . &lt;matplotlib.lines.Line2D at 0x7fdc8aa02eb8&gt; . We can approximate $z_1$ better with 1 + x/2. So . $$ begin{align} y &amp;= x/z_1 y &amp; approx x/(1 + x/2) end{align} $$ And that&#39;s our second order continued fraction approximation! . &lt;matplotlib.lines.Line2D at 0x7fdc8b0f5c50&gt; . Continuing, . $$ begin{align} y &amp; approx x/(1 + x/2) y &amp;= x/(1 + x/z_2) x/y &amp;= 1 + x/z_2 x/y - 1 &amp;= x/z_2 x/(x/y - 1) &amp;= z_2 end{align} $$so we plot and approximate $x/(x/y - 1)$ . &lt;matplotlib.lines.Line2D at 0x7fdc8aa0e320&gt; . and plug it back in to get the third order continued fraction approximation . $$ begin{align} y &amp;= x/(1 + x/z_2) y &amp; approx x/(1 + x/(2 + x/3)) end{align} $$ &lt;matplotlib.lines.Line2D at 0x7fdc8a924e80&gt; . It seems to work pretty well! I don&#39;t have a proof of convergence or anything, but for this example, it seems to produce better approximations than the same order Taylor series. . Here&#39;s a bigger plot of the second order Taylor and continued fraction approximations, together: . &lt;matplotlib.legend.Legend at 0x7fdc8a8997b8&gt; . And the relative error of the second order and third order approximations. In both cases, continued fraction (green) beats Taylor expansion (orange). . &lt;matplotlib.legend.Legend at 0x7fdc8a7ebb70&gt; . We can do this for other functions too! Here&#39;s $y = e^x$ . &lt;matplotlib.legend.Legend at 0x7fdc8a51e828&gt; . And $y = sin(x)$ . &lt;matplotlib.legend.Legend at 0x7fdc8a40e1d0&gt; . In the case of $y = sin(x)$, this seems to be worse than the same order Taylor approximation. Is there a theory for when this method does well? . I searched for information about this and I came across Padé approximant. Does the procedure I&#39;ve described compute a Padé approximant? .",
            "url": "https://luanths.github.io/notes/math/jupyter/2018/06/28/continued-rational-function-approximations.html",
            "relUrl": "/math/jupyter/2018/06/28/continued-rational-function-approximations.html",
            "date": " • Jun 28, 2018"
        }
        
    
  
    
        ,"post7": {
            "title": "Three ways to look at a matrix",
            "content": "What can you do with a matrix? . A matrix can be used to represent at least three different things: . A linear map from vectors to vectors | A bilinear map from pairs of vectors to scalars | A combination of outer products of vectors | The linear map is the most familiar, if you learned linear algebra the way I did. A matrix $M$ represents a linear map which acts according to multiplying a vector by that matrix, $T(v) = Mv$. . For a while I only knew this way to think about matrices, and while it is the right way a lot of the time, sometimes people do use matrices to mean something else and it’s confusing to try to interpret them as linear maps. And sometimes it’s helpful to view the same matrix in multiple ways. So I’d like to write a little about other ways to look at a matrix. . A matrix as a bilinear map . A bilinear map is a function that takes two vectors, returns a vector, and is linear in each argument separately. For this post we’ll only consider bilinear maps that return 1-dimensional vectors, which are the same as scalars. Some familiar examples of these are the dot product and the cross product. . If you have a $m$ by $n$ matrix $M$, you can define a bilinear map $T: mathbb{R}^m times mathbb{R}^n to mathbb{R}$ by: . T(u,v)=uTMvT(u, v) = u^T M vT(u,v)=uTMv . You can think of this as a sort of weighted product of $u$ and $v$, where the $(i,j)$ entry of $M$ is the weight to put on $u_i$ times $v_j$. (Exercise: show this by expanding the matrix-vector products.) . The dot product is what you get if $M = I$. The cross product in three dimensions is given by the matrix . (01−1−1011−10) begin{pmatrix} 0 &amp; 1 &amp; -1 -1 &amp; 0 &amp; 1 1 &amp; -1 &amp; 0 end{pmatrix}⎝⎜⎛​0−11​10−1​−110​⎠⎟⎞​ . A bilinear map can be used to define a quadratic function from vectors to scalars – a quadratic form – by letting $u$ and $v$ be the same vector. This happens, for example, in the probability density of a normal distribution in multiple dimensions: . p(x)∝exp⁡(−(x−μ)TΣ−1(x−μ))p(x) propto exp( -(x - mu)^T Sigma^{-1} (x - mu) )p(x)∝exp(−(x−μ)TΣ−1(x−μ)) . where $ mu$ is the mean and $ Sigma$ is the covariance. The thing in the exponent is a “product” of $x - mu$ with itself, so this equation says that log density peaks at $x = mu$, and it falls off quadratically in each direction at some rate determined by $ Sigma$.1 . If you have a $M$ which represents a linear map $ mathbb{R}^m to mathbb{R}^n$, you can “cast” it into a bilinear map: $u^T M v = u cdot Mv$. That is, first apply the linear map to $v$ to get another vector $M v$, and then take the dot product of that with $u$. . A matrix as a combination of outer products . Given a pair of vectors $u$ and $v$, the outer product $uv^T$ combines the two vectors in a bilinear way. . If you have $uv^T$, you can determine the result of $u^T M v$ for any $M$. This is known as the “trace trick”, because you can prove it by using the trace operator, but you can also see it by writing out the matrix products: . uTMv=∑i,juiMi,jvj=∑i,jMi,juivj=∑i,jMi,j(uvT)i,ju^T M v = sum_{i,j} u_i M_{i,j} v_j = sum_{i,j} M_{i,j} u_i v_j = sum_{i,j} M_{i,j} (uv^T)_{i,j}uTMv=i,j∑​ui​Mi,j​vj​=i,j∑​Mi,j​ui​vj​=i,j∑​Mi,j​(uvT)i,j​ . This also makes it clear that $M$ acts linearly on the elements of $uv^T$. . It is useful to consider the vector space that you get by allowing linear combinations of $uv^T$’s, which one might call the tensor product space2. A member of this space is a matrix that summarizes one or more vector-pairs for the purpose of computing the sum of any bilinear function over those vector-pairs. . The example I have in mind here is covariance matrices. If $x$ is a random vector with zero mean, its covariance matrix is $E[xx^T]$. This matrix lets you compute the expectation of any quadratic function of $x$, that is, anything that looks like $E[x^T M x]$.3 . Some things you can do with matrices and what they mean . Matrix multiplication: . If $A$ and $B$ are matrices representing linear maps, then $AB$ represents the linear map of $A$ composed with the linear map of $B$. . | If $A$ represents a linear map and $B$ represents a bilinear map, $A^T B A$ represents the bilinear map you get by applying $A$ to the each argument before applying $B$. $A^T B$ is the one which applies $A$ to the first argument only, and $B A$ applies $A$ to the second argument only. . | If $A$ and $B$ represent linear maps, $A^T B$ represents the bilinear map you get by applying $A$ to the first argument and $B$ to the second argument, then taking a dot product. . | . Transpose: . If $A$ is a linear map from $U$ to $V$, $A^T$ is a linear map from $V^*$ to $U^*$, where $V^*$ is the dual vector space of $V$: the space of linear maps $V to mathbb{R}$. . | If $A$ is a bilinear map, $A^T$ is the bilinear map you get by switching the arguments. If $A$ is a symmetric matrix, its bilinear map is a symmetric function of its arguments. . | If $A$ is a linear combination of outer products, $A^T$ is what you get if you did the outer products in the other order ($vu^T$ instead of $uv^T$). . | . Eigenvectors: . An eigenvector $v$ of $A$, with eigenvalue $ lambda$, is a vector satisfying $Av = lambda v$. That is, the linear map of $A$ acts on the vector $v$ by scaling it by $ lambda$. . | If $A$ is a bilinear form, a right eigenvector $v$ of $A$ is a vector that has the property that for all $u$, $u^T A v = u^T lambda v = lambda (u cdot v)$. A left eigenvector is the same thing but for the left argument. This property sounds like just a roundabout way to say $Av = lambda v$ but it means that if you have an orthonormal basis of eigenvectors of $A$, it’s easy to describe what $A$ does in that basis. . | . Diagonal: . A linear map whose matrix is a diagonal matrix is one that acts on each coordinate independently. . | A bilinear form whose matrix is diagonal is one that acts on each coordinate of the two vectors independently; it has no “cross terms”. . | . All of this except for diagonality is coordinate free, so it works the same if you change to a different basis. Some of it depends on the dot product, so it only makes sense if it makes sense to take a dot product. I think it’s worth noticing when you do something that depends on your choice of coordinates, or your choice of dot product, because it determines whether you can make these choices arbitrarily or if they matter.4 . Bonus: quantum states and observables . (Disclaimer: I’m not a physicist and I don’t really know what I’m talking about) . In quantum mechanics, an observable is represented by a linear operator $A$ that acts on the quantum state $ psi$. The idea is that you write $ psi$ as a linear combination of eigenstates of $A$, and in each eigenstate the value of $A$ is given by the corresponding eigenvalue, and when you observe $A$ the state collapses to one of the eigenstates. . It always seemed a bit weird to me that the result of the linear operator $A$ doesn’t have a physical meaning and is hardly ever talked about, as if the operator exists only to be a bag of eigenstates. . However, the expectation value of $A$ in the state $ psi$ is $ langle psi | A | psi rangle$ (which is basically physicist’s notation for $ psi^T A psi$). So, maybe another way to think of $A$ is as a bilinear map, which when applied to a quantum state gives the expected value5 of the observable in that state? . If you have an eigendecomposition of $A$, then you can represent any quantum state as a linear combination of eigenstates, so that the expected value of the observable in that state is a linear combination of its expected value in each eigenstate. This explains why eigenstates are important. . This also explains why you want to work with density matrices. The density matrix corresponding to $ psi$ is $| psi rangle langle psi |$, which is just the outer product of $ psi$ with itself. As we’ve seen, this is enough information to determine the expected value of any observable $A$. In density matrix land, you can take linear combinations of pure states to represent mixed states, and in a mixed state the expected value of any observable factors through as a linear combination of its expected value in each of the pure states. . This description depends on the fact that covariance matrices are positive definite, which is basically a condition that the “product” of any vector with itself is not negative. &#8617; . | For more on tensors, see Jeremy Kun’s How to Conquer Tensorphobia and Tensorphobia and the Outer Product. &#8617; . | I like this because it shows that the covariance matrix represents an object that is coordinate-independent; it doesn’t just tell you the covariance between $x_i$ and $x_j$, which you can get by reading off the entries, but also the covariance between linear functions of $x$ that are not coordinate-aligned. &#8617; . | An example that comes to mind is PCA. PCA cares about the Euclidean distances between the points, so it only makes sense when it would make to take dot products. In particular this means that if you apply an arbitrary scaling to some columns of your data, the result of PCA will be different! &#8617; . | There is one weird part of this interpretation: we’ve defined the expected value of an observable in every state, but not its whole distribution in any state. I think you could fix that by having the bilinear map $A$ return the whole probability vector of the observable, rather than just the expected value. &#8617; . |",
            "url": "https://luanths.github.io/notes/math/2018/04/08/matrix-three-ways.html",
            "relUrl": "/math/2018/04/08/matrix-three-ways.html",
            "date": " • Apr 8, 2018"
        }
        
    
  
    
        ,"post8": {
            "title": "Entropy is relative",
            "content": "The entropy of a random quantity $X$ is the “amount of uncertainty” or “information content” in $X$. It is defined as: . H(X)=−E[log⁡P(X)]H(X) = -E[ log P(X) ]H(X)=−E[logP(X)] . It’s the average number of bits you need to specify the value of $X$ if you know its distribution, or equivalently, the average number of random bits you need to generate a sample of $X$. . This makes sense for discrete-valued $X$, like coin flips and letters in English text. But what about continuous distributions? What is the entropy of a normal distribution? . This post is to try to make sense of this. . Infinity? . If you think about it, the entropy of a continuous distribution ought to be infinite. After all, a draw from a normal distribution is a real number which requires an infinite number of digits to describe. . But this isn’t very useful. Somehow a normal distribution with variance 1 seems less uncertain than a normal distribution with variance 2, and we’d like to capture that. . Differential entropy . A first idea is to just reuse the usual definition of entropy, but with probability density instead of mass: . h(X)=−E[log⁡p(X)]h(X) = -E[ log p(X) ]h(X)=−E[logp(X)] . But this is kind of fishy. Probability density can be greater than 1, so $h(X)$ can go negative. Also, the density has units of $1/dx$, but we’re taking the log of it? It turns out that we’ve actually defined the differential entropy, which sort of looks like entropy but doesn’t have all the properties we like. . Relative entropy . Better is the idea of relative entropy (also called KL divergence). If we have a reference distribution with density $q$, then the relative entropy with respect to $q$ is 1 . Hq(X)=−E[log⁡p(X)q(X)]H_q(X) = -E[ log frac{p(X)}{q(X)} ]Hq​(X)=−E[logq(X)p(X)​] . The thing in the log is a ratio of densities, so it’s unitless; at least that checks out. And if you let $q$ be a general measure rather than a distribution, both discrete entropy and differential entropy are special cases of relative entropy (when $q$ is the counting measure or the Lebesgue measure, respectively). . But what does it mean? Relative entropy sometimes described as “the difference between the number of bits to encode $X$ using a code optimized for $q$ and a code optimized for $p$,” but that never made as much sense to me as the intuition for regular entropy does. . The connection . To that end, I found it helpful to think back to regular entropy, interpreting it as the number of random bits you need to sample from the distribution of $X$. . How many bits do you need to sample from a continuous distribution? As I mentioned earlier it’s technically infinity, but no one is going to look at a whole infinite-precision real number anyway. Instead we can ask how many bits are needed to generate $X$ up to $n$ digits of precision. . For now let’s imagine $X$ takes on values from $[0, 1)$, so we can easily represent it as a sequence of binary digits. Then the first $n$ digits is just a binary string, so it has a well defined entropy. . In general this entropy will depend on $n$ in some messy way. But if $X$ has a continuous density, after some point the remaining digits will all be close to uniform, so each additional digit will require an additional random bit. In the limit of large $n$, the number of bits we need will approach $h(X) + n$. . Restating this a bit, we can say that the differential entropy $h(X)$ is the number of bits you need to generate $X$ to the precision of an interval of length $2^{-n}$, minus the number of bits you would need to specify that interval in binary, which is $n$. Cool. . But notice that we’re implicitly using Lebesgue measure here when we talk about interval length and digits of precision. So we can generalize this to relative entropy with respect to any reference measure $q$: it’s the number of bits you need to generate $X$ to the precision a set of measure $2^{-n}$ as measured by $q$. . And now we see that we get regular entropy back when $q$ is the counting measure! If we generate $X$ to within a set of counting measure 1, then we’ve generated $X$ exactly. . Maximum entropy is relative . The maximum entropy distribution is the uniform distribution, because it represents the least state of knowledge about $X$, the state of maximum ignorance. Or is it? . It turns out that the distribution which maximizes relative entropy with respect to $q$ is the one where $p(x)$ is proportional to $q(x)$.2 So the uniform distribution is max entropy only because we’re using a uniform reference measure. . This also solves a puzzle of how the maximum-entropy principle works with change of variables. Example: Suppose you have a coin that lands heads with probability $p$, and you want a prior on $p$. You have no idea, so you assign it the uniform distribution over $[0, 1]$, because it’s the maximum entropy distribution. But what if you instead wanted a prior on $ theta = log p$ instead? You don’t know anything about $ theta$ either, so you ought to assign $ theta$ a maximum entropy uniform distribution over $(- infty, 0]$. But that is a different distribution from the one that makes $p$ uniform, so what gives? . The answer is that you have to fix a reference measure for $p$, which can be either uniform in linear space or log space but not both. The maximum entropy principle can’t help you decide. . Even in the discrete case, there’s a version of this reference measure problem when it’s not clear how to define the set of possible values of a categorical variable. The uniform categorical distribution depends on what the categories are, and categories are made by people. . A word on thermodynamic entropy . Thermodynamic entropy sure doesn’t seem relative. It’s measured in units like joules per kelvin and it seems to have real physical consequences, like the fact that you can’t un-fry an egg. But thermodynamical entropy is supposed to be related to information entropy; can we fit it into the above picture? . As far as I can tell, everything in statistical mechanics depends on the assumption (axiom?) that all microstates of an isolated system in equilibrium are equally probable. That’s a reference measure! . So I think you could say the second law of thermodynamics is really about the relative entropy of the universe, with respect to the reference distribution that is uniform over all microstates. . This definition is non-standard; relative entropy is usually defined as the negative of the above and spelled $D(p Vert q)$ or $KL(p Vert q)$. I defined it this way to draw a stronger analogy to the definition of entropy. &#8617; . | More technically, $p$ has a uniform density with respect to the measure $q$. &#8617; . |",
            "url": "https://luanths.github.io/notes/probability/2018/02/27/entropy.html",
            "relUrl": "/probability/2018/02/27/entropy.html",
            "date": " • Feb 27, 2018"
        }
        
    
  
    
        ,"post9": {
            "title": "Regularizers are not priors",
            "content": "In machine learning, the typical setup is that you have a model with some trainable parameters, and you have some training data, and you fit the model’s parameters to minimize error on the training data. Except usually you don’t directly minimize error on the training data, because you’re wary of overfitting: picking up false patterns in the training sample that don’t hold in the larger population. . . Usually you add regularization, which means that you bias the training process to favor certain model parameters over others – say, those which cause the model to make smoother predictions. . This sounds sort of like having a Bayesian prior over model parameters where the models which make smoother predictions have higher prior probability. And indeed, people often talk about it that way, for instance in scikit-learn’s documentation for ElasticNet. . But a real Bayesian prior is different! In some important ways. This confused me when I was first learning about Bayesian methods, so this post exists to explain the difference. . Running example: regularized least-squares regression . Suppose we have some data consisting of pairs $(x_i, y_i)$ and we would like to fit a model $y = f(x; theta)$.1 . In ordinary least squares, the objective is to find the $ theta$ that minimizes the sum of squared errors: . θ^OLS=arg⁡min⁡θ∑i(yi−f(xi;θ))2 hat theta_{OLS} = arg min_ theta sum_i (y_i - f(x_i; theta))^2θ^OLS​=argθmin​i∑​(yi​−f(xi​;θ))2 . However, this problem may produce an overfit solution or even be underdetermined. . Regularized least squares is when you add an extra term that depends on $ theta$ in order to make the problem more well behaved and produce nicer solutions. One common form of this is L2 regularization or ridge regression, which adds a penalty term on the L2 norm of $ theta$: . θ^Ridge=arg⁡min⁡θ(∑i(yi−f(xi;θ))2+∥θ∥2) hat theta_{Ridge} = arg min_ theta left( sum_i (y_i - f(x_i; theta))^2 + Vert theta Vert^2 right)θ^Ridge​=argθmin​(i∑​(yi​−f(xi​;θ))2+∥θ∥2) . This can make sense if $ theta$ is a vector of coefficients: it discourages large coefficients that happen to cancel out near the observed data points but produce wild predictions elsewhere, such as in the intro figure. . The connection to probability . So far we haven’t said anything about probability. However, it turns out we can cast our modeling in a probabilistic light by interpreting the loss as a negative log likelihood. . In particular, let’s now imagine our model is probabilistic, so that $y$ follows a distribution that depends on $x$. If we let2 . p(y∣x;θ)∝e−(y−f(x;θ))2p(y mid x; theta) propto e^{-(y - f(x; theta))^2}p(y∣x;θ)∝e−(y−f(x;θ))2 . then the least squares objective from before boils down to . θ^ML=arg⁡max⁡θp(y∣x;θ) hat theta_{ML} = arg max_ theta p(y mid x; theta)θ^ML​=argθmax​p(y∣x;θ) . This is called maximum likelihood. Pretty intuitive: find the $ theta$ which assigns the highest probability to the data. . In this framework, we can incorporate the regularization term by imagining we have a prior on $ theta$, . p(θ)∝e−∥θ∥2p( theta) propto e^{- Vert theta Vert^2}p(θ)∝e−∥θ∥2 . And now we’re maximizing $p(y mid x; theta) p( theta)$. This is called maximum a posteriori estimation or MAP for short. . θ^MAP=arg⁡max⁡θp(y∣x;θ)p(θ) hat theta_{MAP} = arg max_ theta p(y mid x; theta) p( theta)θ^MAP​=argθmax​p(y∣x;θ)p(θ) . This is what people mean when they say, for example, that L2 regularization corresponds to a prior on the coefficients. . So are we Bayes now? . It’s not the same . The biggest way in which this is different from fully Bayesian updating is that it produces a point estimate, a single value $ hat theta$. . In Bayesian inference, the result of updating on new data is the posterior distribution of $ theta$. The posterior represents your belief state after seeing the data; different points in that distribution are different model parameters consistent with the data; the shape of the distribution captures uncertainty about the model parameters. The posterior becomes your new prior going forward, and additional data can further inform your belief about the correct model. At no point does your belief state collapse to a single value of $ theta$. . Let’s say after having observed all the $(x_i, y_i)$ we want to make a prediction at a new point $x^*$. . The classical thing to do, whether you’re including a regularizer or not, would be to use the $ hat theta$ from your learning algorithm to predict $y^* = f(x^*; hat theta)$. . The fully Bayesian thing to do would be to integrate over your posterior distribution $p( theta mid D)$, obtaining a predictive distribution on $y^*$: . p(y∗∣x∗,D)=∫p(y∗∣x∗,θ)p(θ∣D)dθp(y^* mid x^*, D) = int p(y^* mid x^*, theta) p( theta mid D) d thetap(y∗∣x∗,D)=∫p(y∗∣x∗,θ)p(θ∣D)dθ . This takes into account your uncertainty about $ theta$ and “pushes it forward” into uncertainty about $y^*$ by averaging over predictions made by different plausible $ theta$ values. . Obviously in many cases this is a less tractable thing to do. But it does get at something that the point estimate doesn’t, and sometimes it may be worth trying to do something more like the Bayesian approach. . Machine learning researchers have been trying to do this! Here are some blog posts about applying Bayesian techniques to deep models. . Alex Kendall: Deep Learning Is Not Good Enough, We Need Bayesian Deep Learning for Safe AI (2017) | Yarin Gal: What My Deep Model Doesn’t Know (2015) | Andrew Gordon Wilson: The Case for Bayesian Deep Learning (2020) | . More things about Bayes and non-Bayes . MAP estimation, the thing where you find the maximum of the posterior and go with it, is not well defined under change of variables. If you have a model parametrized by $ theta$, and you decide, say, to set $u = 1/ theta$ and optimize $u$ instead, the posterior max $ hat u_{MAP}$ need not be equal to $1/ hat theta_{MAP}$. . In contrast, Bayes doesn’t care what variables you choose to represent the model, as long as you take care to transform your prior density function appropriately for the change of variables.3 . | . If you go back and look at Bayes’ rule, there’s one piece we didn’t about: the normalization constant. . p(D)=∫p(D∣θ)p(θ)dθp(D) = int p(D mid theta) p( theta) d thetap(D)=∫p(D∣θ)p(θ)dθ This factor is safe to ignore when optimizing over $ theta$. But it has a nice interpretation: it’s the probability of seeing the data $D$, averaged over the prior $p( theta)$. This is sometimes called the model evidence, and you can think of it as measuring how good your prior was in light of seeing the data. You can even use it to compare different priors.4 . | . Probability distributions have to sum up to one. This relates to the previous point. Thinking of your prior as an actual probability distribution forces you to not assign more probability mass than you have. Compared to inventing a loss function out of thin air and minimizing it, sticking to a probabilistic framework feels to me like almost a form of type checking that rules out some “priors” that make no sense. | . In summary my point here is that the Bayesian approach brings more to the table and is not just the same as adding a regularization term. I’m not saying Bayes is the appropriate framework for every problem, but I often find its answers intuitively pleasing and I think it’s certainly worth learning more about. . Here, I mean that $f$ is some family of models parameterized by $ theta$. For example, $f$ could be the family of 9th-order polynomials, and $ theta$ the coefficients of the polynomial. &#8617; . | You may know this as the density of a normal distribution centered around $y = f(x; theta)$. &#8617; . | Max likelihood also doesn’t care, since there’s no prior at all. Also, if $ theta$ is discrete, then MAP is well defined again: it’s the one value of $ theta$ with the most probability mass. It’s still only as meaningful as a plurality winner in a vote, though. &#8617; . | For more on this kind of stuff, I recommend David MacKay’s textbook Inference Theory, Inference, and Learning Algorithms, in particular chapter 3 (“More about Inference”) and chapter 28 (“Model Comparison and Occam’s Razor”). &#8617; . |",
            "url": "https://luanths.github.io/notes/probability/2018/02/22/not-priors.html",
            "relUrl": "/probability/2018/02/22/not-priors.html",
            "date": " • Feb 22, 2018"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I like learning, thinking about puzzles, and making things better. I currently work in quant trading in NYC. .",
          "url": "https://luanths.github.io/notes/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://luanths.github.io/notes/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}