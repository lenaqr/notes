{
  
    
        "post0": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://luanths.github.io/notes/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://luanths.github.io/notes/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Regularizers are not priors",
            "content": "Regularizers are not priors . In machine learning, the typical setup is that you have a model with some trainable parameters, and you have some training data, and you fit the model’s parameters to minimize error on the training data. Except usually you don’t directly minimize error on the training data, because you’re wary of overfitting: picking up false patterns in the training sample that don’t hold in the larger population. . . Usually you add regularization, which means that you bias the training process to favor certain model parameters over others – say, those which cause the model to make smoother predictions. . This sounds sort of like having a Bayesian prior over model parameters where the models which make smoother predictions have higher prior probability. And indeed, people often talk about it that way, for instance in scikit-learn’s documentation for ElasticNet. . But a real Bayesian prior is different! In some important ways. This confused me when I was first learning about Bayesian methods, so this post exists to explain the difference. . Running example: regularized least-squares regression . Suppose we have some data consisting of pairs $(x_i, y_i)$ and we would like to fit a model $y = f(x; theta)$.1 . In ordinary least squares, the objective is to find the $ theta$ that minimizes the sum of squared errors: . θ^OLS=arg⁡min⁡θ∑i(yi−f(xi;θ))2 hat theta_{OLS} = arg min_ theta sum_i (y_i - f(x_i; theta))^2θ^OLS​=argθmin​i∑​(yi​−f(xi​;θ))2 . However, this problem may produce an overfit solution or even be underdetermined. . Regularized least squares is when you add an extra term that depends on $ theta$ in order to make the problem more well behaved and produce nicer solutions. One common form of this is L2 regularization or ridge regression, which adds a penalty term on the L2 norm of $ theta$: . θ^Ridge=arg⁡min⁡θ(∑i(yi−f(xi;θ))2+∥θ∥2) hat theta_{Ridge} = arg min_ theta left( sum_i (y_i - f(x_i; theta))^2 + Vert theta Vert^2 right)θ^Ridge​=argθmin​(i∑​(yi​−f(xi​;θ))2+∥θ∥2) . This can make sense if $ theta$ is a vector of coefficients: it discourages large coefficients that happen to cancel out near the observed data points but produce wild predictions elsewhere, such as in the intro figure. . The connection to probability . So far we haven’t said anything about probability. However, it turns out we can cast our modeling in a probabilistic light by interpreting the loss as a negative log likelihood. . In particular, let’s now imagine our model is probabilistic, so that $y$ follows a distribution that depends on $x$. If we let2 . p(y∣x;θ)∝e−(y−f(x;θ))2p(y mid x; theta) propto e^{-(y - f(x; theta))^2}p(y∣x;θ)∝e−(y−f(x;θ))2 . then the least squares objective from before boils down to . θ^ML=arg⁡max⁡θp(y∣x;θ) hat theta_{ML} = arg max_ theta p(y mid x; theta)θ^ML​=argθmax​p(y∣x;θ) . This is called maximum likelihood. Pretty intuitive: find the $ theta$ which assigns the highest probability to the data. . In this framework, we can incorporate the regularization term by imagining we have a prior on $ theta$, . p(θ)∝e−∥θ∥2p( theta) propto e^{- Vert theta Vert^2}p(θ)∝e−∥θ∥2 . And now we’re maximizing $p(y mid x; theta) p( theta)$. This is called maximum a posteriori estimation or MAP for short. . θ^MAP=arg⁡max⁡θp(y∣x;θ)p(θ) hat theta_{MAP} = arg max_ theta p(y mid x; theta) p( theta)θ^MAP​=argθmax​p(y∣x;θ)p(θ) . This is what people mean when they say, for example, that L2 regularization corresponds to a prior on the coefficients. . So are we Bayes now? . It’s not the same . The biggest way in which this is different from fully Bayesian updating is that it produces a point estimate, a single value $ hat theta$. . In Bayesian inference, the result of updating on new data is the posterior distribution of $ theta$. The posterior represents your belief state after seeing the data; different points in that distribution are different model parameters consistent with the data; the shape of the distribution captures uncertainty about the model parameters. The posterior becomes your new prior going forward, and additional data can further inform your belief about the correct model. At no point does your belief state collapse to a single value of $ theta$. . Let’s say after having observed all the $(x_i, y_i)$ we want to make a prediction at a new point $x^*$. . The classical thing to do, whether you’re including a regularizer or not, would be to use the $ hat theta$ from your learning algorithm to predict $y^* = f(x^*; hat theta)$. . The fully Bayesian thing to do would be to integrate over your posterior distribution $p( theta mid D)$, obtaining a predictive distribution on $y^*$: . p(y∗∣x∗,D)=∫p(y∗∣x∗,θ)p(θ∣D)dθp(y^* mid x^*, D) = int p(y^* mid x^*, theta) p( theta mid D) d thetap(y∗∣x∗,D)=∫p(y∗∣x∗,θ)p(θ∣D)dθ . This takes into account your uncertainty about $ theta$ and “pushes it forward” into uncertainty about $y^*$ by averaging over predictions made by different plausible $ theta$ values. . Obviously in many cases this is a less tractable thing to do. But it does get at something that the point estimate doesn’t, and sometimes it may be worth trying to do something more like the Bayesian approach. . Machine learning researchers have been trying to do this! Here are some blog posts about applying Bayesian techniques to deep models. . Alex Kendall: Deep Learning Is Not Good Enough, We Need Bayesian Deep Learning for Safe AI (2017) | Yarin Gal: What My Deep Model Doesn’t Know (2015) | Andrew Gordon Wilson: The Case for Bayesian Deep Learning (2020) | . More things about Bayes and non-Bayes . MAP estimation, the thing where you find the maximum of the posterior and go with it, is not well defined under change of variables. If you have a model parametrized by $ theta$, and you decide, say, to set $u = 1/ theta$ and optimize $u$ instead, the posterior max $ hat u_{MAP}$ need not be equal to $1/ hat theta_{MAP}$. . In contrast, Bayes doesn’t care what variables you choose to represent the model, as long as you take care to transform your prior density function appropriately for the change of variables.3 . | . If you go back and look at Bayes’ rule, there’s one piece we didn’t about: the normalization constant. . p(D)=∫p(D∣θ)p(θ)dθp(D) = int p(D mid theta) p( theta) d thetap(D)=∫p(D∣θ)p(θ)dθ This factor is safe to ignore when optimizing over $ theta$. But it has a nice interpretation: it’s the probability of seeing the data $D$, averaged over the prior $p( theta)$. This is sometimes called the model evidence, and you can think of it as measuring how good your prior was in light of seeing the data. You can even use it to compare different priors.4 . | . Probability distributions have to sum up to one. This relates to the previous point. Thinking of your prior as an actual probability distribution forces you to not assign more probability mass than you have. Compared to inventing a loss function out of thin air and minimizing it, sticking to a probabilistic framework feels to me like almost a form of type checking that rules out some “priors” that make no sense. | . In summary my point here is that the Bayesian approach brings more to the table and is not just the same as adding a regularization term. I’m not saying Bayes is the appropriate framework for every problem, but I often find its answers intuitively pleasing and I think it’s certainly worth learning more about. . Here, I mean that $f$ is some family of models parameterized by $ theta$. For example, $f$ could be the family of 9th-order polynomials, and $ theta$ the coefficients of the polynomial. &#8617; . | You may know this as the density of a normal distribution centered around $y = f(x; theta)$. &#8617; . | Max likelihood also doesn’t care, since there’s no prior at all. Also, if $ theta$ is discrete, then MAP is well defined again: it’s the one value of $ theta$ with the most probability mass. It’s still only as meaningful as a plurality winner in a vote, though. &#8617; . | For more on this kind of stuff, I recommend David MacKay’s textbook Inference Theory, Inference, and Learning Algorithms, in particular chapter 3 (“More about Inference”) and chapter 28 (“Model Comparison and Occam’s Razor”). &#8617; . |",
            "url": "https://luanths.github.io/notes/probability/2018/02/22/not-priors.html",
            "relUrl": "/probability/2018/02/22/not-priors.html",
            "date": " • Feb 22, 2018"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I like learning, thinking about puzzles, and making things better. I currently work in quant trading in NYC. .",
          "url": "https://luanths.github.io/notes/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://luanths.github.io/notes/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}