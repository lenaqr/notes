<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Entropy is relative | Lu’s Notes</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Entropy is relative" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Trying to make sense of the entropy of a continuous distribution." />
<meta property="og:description" content="Trying to make sense of the entropy of a continuous distribution." />
<link rel="canonical" href="https://luanths.github.io/notes/probability/2018/02/27/entropy.html" />
<meta property="og:url" content="https://luanths.github.io/notes/probability/2018/02/27/entropy.html" />
<meta property="og:site_name" content="Lu’s Notes" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-02-27T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"Trying to make sense of the entropy of a continuous distribution.","@type":"BlogPosting","url":"https://luanths.github.io/notes/probability/2018/02/27/entropy.html","headline":"Entropy is relative","dateModified":"2018-02-27T00:00:00-06:00","datePublished":"2018-02-27T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://luanths.github.io/notes/probability/2018/02/27/entropy.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/notes/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://luanths.github.io/notes/feed.xml" title="Lu's Notes" /><link rel="shortcut icon" type="image/x-icon" href="/notes/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Entropy is relative | Lu’s Notes</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Entropy is relative" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Trying to make sense of the entropy of a continuous distribution." />
<meta property="og:description" content="Trying to make sense of the entropy of a continuous distribution." />
<link rel="canonical" href="https://luanths.github.io/notes/probability/2018/02/27/entropy.html" />
<meta property="og:url" content="https://luanths.github.io/notes/probability/2018/02/27/entropy.html" />
<meta property="og:site_name" content="Lu’s Notes" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-02-27T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"Trying to make sense of the entropy of a continuous distribution.","@type":"BlogPosting","url":"https://luanths.github.io/notes/probability/2018/02/27/entropy.html","headline":"Entropy is relative","dateModified":"2018-02-27T00:00:00-06:00","datePublished":"2018-02-27T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://luanths.github.io/notes/probability/2018/02/27/entropy.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inconsolata|Merriweather"><link type="application/atom+xml" rel="alternate" href="https://luanths.github.io/notes/feed.xml" title="Lu's Notes" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/notes/">Lu&#39;s Notes</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/notes/about/">About Me</a><a class="page-link" href="/notes/search/">Search</a><a class="page-link" href="/notes/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Entropy is relative</h1><p class="page-description">Trying to make sense of the entropy of a continuous distribution.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2018-02-27T00:00:00-06:00" itemprop="datePublished">
        Feb 27, 2018
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      6 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/notes/categories/#probability">probability</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>The entropy of a random quantity $X$ is the “amount of uncertainty” or
“information content” in $X$. It is defined as:</p>

<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mi>E</mi><mo stretchy="false">[</mo><mi>log</mi><mo>⁡</mo><mi>P</mi><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">H(X) = -E[ \log P(X) ]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">−</span><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="mopen">[</span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mclose">)</span><span class="mclose">]</span></span></span></span></span>

<p>It’s the average number of bits you need to specify the value of $X$ if you know
its distribution, or equivalently, the average number of random bits you need to
generate a sample of $X$.</p>

<p>This makes sense for discrete-valued $X$, like coin flips and letters in English
text. But what about continuous distributions? What is the entropy of a normal
distribution?</p>

<p>This post is to try to make sense of this.</p>

<h2 id="infinity">Infinity?</h2>

<p>If you think about it, the entropy of a continuous distribution ought to be
infinite. After all, a draw from a normal distribution is a real number which
requires an infinite number of digits to describe.</p>

<p>But this isn’t very useful. Somehow a normal distribution with variance 1 seems
less uncertain than a normal distribution with variance 2, and we’d like to
capture that.</p>

<h2 id="differential-entropy">Differential entropy</h2>

<p>A first idea is to just reuse the usual definition of entropy, but with
probability density instead of mass:</p>

<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mi>E</mi><mo stretchy="false">[</mo><mi>log</mi><mo>⁡</mo><mi>p</mi><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">h(X) = -E[ \log p(X) ]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">h</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">−</span><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="mopen">[</span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">p</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mclose">)</span><span class="mclose">]</span></span></span></span></span>

<p>But this is kind of fishy. Probability density can be greater than 1, so $h(X)$
can go negative. Also, the density has units of $1/dx$, but we’re taking the log
of it? It turns out that we’ve actually defined the <a href="https://en.wikipedia.org/wiki/Differential_entropy">differential
entropy</a>, which sort of looks like entropy but doesn’t have all the
properties we like.</p>

<h2 id="relative-entropy">Relative entropy</h2>

<p>Better is the idea of relative entropy (also called <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">KL divergence</a>). If we
have a reference distribution with density $q$, then the relative entropy with
respect to $q$ is <sup id="fnref:nonstandard"><a href="#fn:nonstandard" class="footnote">1</a></sup></p>

<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>H</mi><mi>q</mi></msub><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mi>E</mi><mo stretchy="false">[</mo><mi>log</mi><mo>⁡</mo><mfrac><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo></mrow><mrow><mi>q</mi><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo></mrow></mfrac><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">H_q(X) = -E[ \log \frac{p(X)}{q(X)} ]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.363em;vertical-align:-0.936em;"></span><span class="mord">−</span><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="mopen">[</span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">]</span></span></span></span></span>

<p>The thing in the log is a ratio of densities, so it’s unitless; at least that
checks out. And if you let $q$ be a general measure rather than a distribution,
both discrete entropy and differential entropy are special cases of relative
entropy (when $q$ is the counting measure or the Lebesgue measure,
respectively).</p>

<p>But what does it mean? Relative entropy sometimes described as “the difference
between the number of bits to encode $X$ using a code optimized for $q$ and a
code optimized for $p$,” but that never made as much sense to me as the
intuition for regular entropy does.</p>

<h2 id="the-connection">The connection</h2>

<p>To that end, I found it helpful to think back to regular entropy, interpreting
it as the number of random bits you need to sample from the distribution of $X$.</p>

<p>How many bits do you need to sample from a continuous distribution? As I
mentioned earlier it’s technically infinity, but no one is going to look at a
whole infinite-precision real number anyway. Instead we can ask how many bits
are needed to generate $X$ up to $n$ digits of precision.</p>

<p>For now let’s imagine $X$ takes on values from $[0, 1)$, so we can easily
represent it as a sequence of binary digits. Then the first $n$ digits is just a
binary string, so it has a well defined entropy.</p>

<p>In general this entropy will depend on $n$ in some messy way. But if $X$ has a
continuous density, after some point the remaining digits will all be close to
uniform, so each additional digit will require an additional random bit. In the
limit of large $n$, the number of bits we need will approach $h(X) + n$.</p>

<p>Restating this a bit, we can say that the differential entropy $h(X)$ is the
number of bits you need to generate $X$ to the precision of an interval of
length $2^{-n}$, minus the number of bits you would need to specify that
interval in binary, which is $n$. Cool.</p>

<p>But notice that we’re implicitly using Lebesgue measure here when we talk about
interval length and digits of precision. So we can generalize this to relative
entropy with respect to any reference measure $q$: it’s the number of bits you
need to generate $X$ to the precision a set of measure $2^{-n}$ as measured by
$q$.</p>

<p>And now we see that we get regular entropy back when $q$ is the counting
measure! If we generate $X$ to within a set of counting measure 1, then we’ve
generated $X$ exactly.</p>

<h2 id="maximum-entropy-is-relative">Maximum entropy is relative</h2>

<p>The maximum entropy distribution is the uniform distribution, because it
represents the least state of knowledge about $X$, the state of maximum
ignorance. Or is it?</p>

<p>It turns out that the distribution which maximizes relative entropy with respect
to $q$ is the one where $p(x)$ is proportional to $q(x)$.<sup id="fnref:proportional"><a href="#fn:proportional" class="footnote">2</a></sup> So the
uniform distribution is max entropy only because we’re using a uniform reference
measure.</p>

<p>This also solves a puzzle of how the <a href="https://en.wikipedia.org/wiki/Principle_of_maximum_entropy">maximum-entropy principle</a> works
with change of variables. Example: Suppose you have a coin that lands heads with
probability $p$, and you want a prior on $p$. You have no idea, so you assign it
the uniform distribution over $[0, 1]$, because it’s the maximum entropy
distribution. But what if you instead wanted a prior on $\theta = \log p$
instead? You don’t know anything about $\theta$ either, so you ought to assign
$\theta$ a maximum entropy uniform distribution over $(-\infty, 0]$. But that is
a different distribution from the one that makes $p$ uniform, so what gives?</p>

<p>The answer is that you have to fix a reference measure for $p$, which can be
either uniform in linear space or log space but not both. The maximum entropy
principle can’t help you decide.</p>

<p>Even in the discrete case, there’s a version of this reference measure problem
when it’s not clear how to define the set of possible values of a categorical
variable. The uniform categorical distribution depends on what the categories
are, and categories are made by people.</p>

<h2 id="a-word-on-thermodynamic-entropy">A word on thermodynamic entropy</h2>

<p>Thermodynamic entropy sure doesn’t seem relative. It’s measured in units like
joules per kelvin and it seems to have real physical consequences, like the fact
that you can’t un-fry an egg. But thermodynamical entropy is supposed to be
related to information entropy; can we fit it into the above picture?</p>

<p>As far as I can tell, everything in statistical mechanics depends on the
assumption (axiom?) that all microstates of an <a href="https://ocw.mit.edu/courses/physics/8-044-statistical-physics-i-spring-2013/readings-notes-slides/MIT8_044S13_mcrocanoncl.pdf">isolated system in
equilibrium</a> are equally probable. That’s a reference measure!</p>

<p>So I think you could say the second law of thermodynamics is really about the
<em>relative</em> entropy of the universe, with respect to the reference distribution
that is uniform over all microstates.</p>
<div class="footnotes">
  <ol>
    <li id="fn:nonstandard">
      <p>This definition is non-standard; relative entropy is usually
defined as the negative of the above and spelled $D(p \Vert q)$ or $KL(p
\Vert q)$. I defined it this way to draw a stronger analogy to the
definition of entropy. <a href="#fnref:nonstandard" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:proportional">
      <p>More technically, $p$ has a uniform density with respect to the measure $q$. <a href="#fnref:proportional" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="luanths/notes"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/notes/probability/2018/02/27/entropy.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/notes/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/notes/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/notes/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p></p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/luanths" title="luanths"><svg class="svg-icon grey"><use xlink:href="/notes/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/luanths" title="luanths"><svg class="svg-icon grey"><use xlink:href="/notes/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
